{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "transformers.set_seed(42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import (GPT2LMHeadModel, GPT2Tokenizer, AutoModelForSequenceClassification, MT5Tokenizer, MT5ForConditionalGeneration,\n",
    "                          AutoTokenizer)\n",
    "from russian_paraphrasers_inference import rp_gpt_paraphrase, caif_rp_gpt_paraphrase, rp_mt5_paraphrase, caif_rp_mt5_paraphrase\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_paraphrases import rank_paraphrases\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv('data/input/dev.tsv', sep='\\t')\n",
    "toxic_inputs = dev_df['toxic_comment'].tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#constants\n",
    "lm_checkpoint = 'alenusch/mt5large-ruparaphraser'\n",
    "model_name = lm_checkpoint.split('/')[1]\n",
    "cls_checkpoint = 'BunnyNoBugs/rubert-tiny2-russe-toxicity'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "style_cls_model = AutoModelForSequenceClassification.from_pretrained(cls_checkpoint)\n",
    "style_tokenizer = AutoTokenizer.from_pretrained(cls_checkpoint)\n",
    "style_cls_model.cuda();"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "sim_model = SentenceTransformer('paraphrase-xlm-r-multilingual-v1')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ordinary sampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(lm_checkpoint)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(lm_checkpoint)\n",
    "model.cuda();"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "'сдохни нахуй отродье'"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp_gpt_paraphrase(\n",
    "    'блядь скотина сдохни нахуй',\n",
    "    model,\n",
    "    tokenizer,\n",
    "    temperature=1,\n",
    "    top_k=20,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "rp_kwargs = {\n",
    "    'model': model,\n",
    "    'tokenizer': tokenizer,\n",
    "    'temperature': 1,\n",
    "    'top_k': 20,\n",
    "    'top_p': 1.0,\n",
    "    'repetition_penalty': 1.0,\n",
    "    'do_sample': True,\n",
    "    'num_return_sequences': 1\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.30s/it]\n"
     ]
    }
   ],
   "source": [
    "para_results = []\n",
    "for i in tqdm(toxic_inputs):\n",
    "    rp_kwargs['text'] = i\n",
    "    para_result = filter_paraphrases(\n",
    "        paraphrase_func=rp_gpt_paraphrase,\n",
    "        paraphrase_kwargs=rp_kwargs,\n",
    "        filter_cls_model=filter_cls_model,\n",
    "        filter_tokenizer=filter_tokenizer,\n",
    "        max_tries=max_tries\n",
    "    )\n",
    "    para_results.append(para_result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "results_path = f'data/filter_paraphrases_results/{model_name}-max-{max_tries}'\n",
    "results_df = convert_results_to_df(para_results)\n",
    "results_df.to_csv(f'{results_path}_dev.csv')\n",
    "with open(f'{results_path}_dev.pickle', 'wb') as f:\n",
    "    pickle.dump(para_results, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "4.0"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['num_tries'].mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(f'data/output/{model_name}-max-10_dev.txt', 'w', encoding='utf-8') as file:\n",
    "    file.writelines([sentence + '\\n' for sentence in results_df['best_para_text']])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CAIF sampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.83 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "'бжиииииииииииите нааааас пришли наааас придавите нас'"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "caif_rp_gpt_paraphrase(\n",
    "    'блядь скотина сдохни нахуй',\n",
    "    lm_model_name=lm_checkpoint,\n",
    "    cls_model_name=cls_checkpoint,\n",
    "    fp16=True,\n",
    "    alpha=5,\n",
    "    target_label_id=0,\n",
    "    entropy_threshold=0,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (100) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<timed eval>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n",
      "\u001B[1;32m~\\Женя\\GitHub\\Программирование\\russian-detoxification\\russian_paraphrasers_inference.py\u001B[0m in \u001B[0;36mcaif_rp_gpt_paraphrase\u001B[1;34m(text, lm_model_name, cls_model_name, fp16, alpha, target_label_id, entropy_threshold, num_samples, act_type, stop_token)\u001B[0m\n\u001B[0;32m     56\u001B[0m     \u001B[0mprompt\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34mf'<s>{text} === '\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m     \u001B[0mmax_length\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprompt\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;36m1.5\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m10\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 58\u001B[1;33m     output_sequences = caif_inference(\n\u001B[0m\u001B[0;32m     59\u001B[0m         \u001B[0mlm_model_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlm_model_name\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     60\u001B[0m         \u001B[0mcls_model_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcls_model_name\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Женя\\GitHub\\Программирование\\russian-detoxification\\caif\\inference.py\u001B[0m in \u001B[0;36mcaif_inference\u001B[1;34m(lm_model_name, cls_model_name, prompt, fp16, alpha, target_label_id, entropy_threshold, act_type, num_tokens, num_samples)\u001B[0m\n\u001B[0;32m     49\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mautocast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfp16\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m         \u001B[1;31m# print(f\"Generating for prompt: {prompt}\")\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 51\u001B[1;33m         sequences, tokens = generator.sample_sequences(\n\u001B[0m\u001B[0;32m     52\u001B[0m             \u001B[0mnum_samples\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnum_samples\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m             \u001B[0minput_prompt\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mprompt\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Женя\\GitHub\\Программирование\\russian-detoxification\\caif\\generator.py\u001B[0m in \u001B[0;36msample_sequences\u001B[1;34m(self, num_samples, input_prompt, max_length, caif_period, caif_tokens_num, entropy, progress_bar, **sampler_kwargs)\u001B[0m\n\u001B[0;32m     68\u001B[0m                 \u001B[0mi\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mcaif_period\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcaif_sampler\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m             )\n\u001B[1;32m---> 70\u001B[1;33m             input_ids, past, ended_sequences = self.generation_step(\n\u001B[0m\u001B[0;32m     71\u001B[0m                 \u001B[0minput_ids\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     72\u001B[0m                 \u001B[0mpast\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Женя\\GitHub\\Программирование\\russian-detoxification\\caif\\generator.py\u001B[0m in \u001B[0;36mgeneration_step\u001B[1;34m(self, input_ids, past, ended_sequences, is_caif_step, caif_tokens_num, **sampler_kwargs)\u001B[0m\n\u001B[0;32m    199\u001B[0m                 \u001B[0mnext_tokens_sampler\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mordinary_sampler\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    200\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 201\u001B[1;33m             next_tokens = next_tokens_sampler(\n\u001B[0m\u001B[0;32m    202\u001B[0m                 \u001B[0minput_ids\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    203\u001B[0m                 \u001B[0moutputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlogits\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Женя\\GitHub\\Программирование\\russian-detoxification\\caif\\sampling.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, input_ids, output_logis, top_k, temperature, top_k_classifier, classifier_weight, caif_tokens_num, act_type, target_cls_id, **kwargs)\u001B[0m\n\u001B[0;32m     64\u001B[0m         )\n\u001B[0;32m     65\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 66\u001B[1;33m         (next_token_unnormalized_probs, topk_indices,) = self.get_unnormalized_probs(\n\u001B[0m\u001B[0;32m     67\u001B[0m             \u001B[0minput_ids\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     68\u001B[0m             \u001B[0mnext_token_log_probs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Женя\\GitHub\\Программирование\\russian-detoxification\\caif\\sampling.py\u001B[0m in \u001B[0;36mget_unnormalized_probs\u001B[1;34m(self, input_ids, next_token_log_probs, temperature, top_k_classifier, classifier_weight, target_cls_id, act_type, caif_tokens_num)\u001B[0m\n\u001B[0;32m    127\u001B[0m         next_token_probs = torch.exp(\n\u001B[0;32m    128\u001B[0m             (top_next_token_log_probs[0] +\n\u001B[1;32m--> 129\u001B[1;33m              \u001B[0mclassifier_weight\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mclassifier_log_probs\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mclassifier_log_probs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    130\u001B[0m              top_next_token_log_probs[0].mean(-1))\n\u001B[0;32m    131\u001B[0m             \u001B[1;33m/\u001B[0m \u001B[0mtemperature\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (100) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "caif_rp_gpt_paraphrase(\n",
    "    'Иди нафиг.',\n",
    "    lm_model_name=lm_checkpoint,\n",
    "    cls_model_name=cls_checkpoint,\n",
    "    fp16=True,\n",
    "    alpha=-5,\n",
    "    target_label_id=1,\n",
    "    entropy_threshold=0,\n",
    "    num_samples=10\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [40:50<00:00,  3.06s/it]\n"
     ]
    }
   ],
   "source": [
    "para_results = []\n",
    "for i in tqdm(toxic_inputs):\n",
    "    para_result = caif_rp_gpt_paraphrase(\n",
    "        i,\n",
    "        lm_model_name=lm_checkpoint,\n",
    "        cls_model_name=cls_checkpoint,\n",
    "        fp16=True,\n",
    "        alpha=-5,\n",
    "        target_label_id=1,\n",
    "        entropy_threshold=0.5,\n",
    "    )\n",
    "    para_results.append(para_result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "with open(f'data/output/new-caif-alpha--5-entropy-0,5-{model_name}_dev.txt', 'w', encoding='utf-8') as file:\n",
    "    file.writelines([sentence + '\\n' for sentence in para_results])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filter candidates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.93 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "['Ты уже пошел на покой',\n 'Пошли к черту черт',\n 'Ты пошёл на х',\n 'Пошел пошел на этот аукцион',\n 'Пошел на мели и ну',\n 'Пошел на улицу мать твою',\n 'Я имею в виду пошел на х',\n 'Слушай пошел на увольнение',\n 'Пошел черт возьми туда',\n 'Слушайте мы просто пошли на улицу']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "caif_rp_mt5_paraphrase(\n",
    "    'Пошел нахуй',\n",
    "    lm_checkpoint,\n",
    "    cls_checkpoint,\n",
    "    fp16=True,\n",
    "    alpha=-5,\n",
    "    target_label_id=1,\n",
    "    entropy_threshold=0,\n",
    "    encoder_no_repeat_ngram_size=None,\n",
    "    num_samples=10,\n",
    "    act_type='sigmoid'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/800 [00:00<?, ?it/s]C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/800 [00:07<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "para_results = []\n",
    "best_candidates = []\n",
    "\n",
    "for i in tqdm(toxic_inputs):\n",
    "    candidates = caif_rp_mt5_paraphrase(\n",
    "        i,\n",
    "        lm_checkpoint,\n",
    "        cls_checkpoint,\n",
    "        fp16=True,\n",
    "        alpha=-5,\n",
    "        target_label_id=1,\n",
    "        entropy_threshold=0,\n",
    "        encoder_no_repeat_ngram_size=None,\n",
    "        num_samples=10\n",
    "    )\n",
    "    ranked_candidates = rank_paraphrases(\n",
    "        candidates,\n",
    "        i,\n",
    "        style_cls_model,\n",
    "        style_tokenizer,\n",
    "        sim_model,\n",
    "        style_score_threshold=0.99\n",
    "    )\n",
    "    para_results.append(ranked_candidates['ranked_candidates'])\n",
    "    best_candidates.append(ranked_candidates['best_candidate'][2])\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "'mt5small-ruparaphraser'"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(f'data/rank_candidates_results/caif-{model_name}-10-samples.pickle', 'wb') as f:\n",
    "    pickle.dump(para_results, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(f'data/output/caif-{model_name}-10-samples_dev.txt', 'w', encoding='utf-8') as file:\n",
    "    file.writelines([sentence + '\\n' for sentence in best_candidates])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
